import torch
import torch.nn as nn
from models.model_def import ModelFactory
from preprocess.preprocessor import DataPreprocessor
from typing import Dict, Any, Union, List
import numpy as np

class ModelLoader:
    """æ¨¡å‹åŠ è½½å™¨ï¼Œç”¨äºåŠ è½½å’Œè¿è¡Œè®­ç»ƒå¥½çš„æ¨¡å‹"""
    
    def __init__(self, model_path: str):
        """åˆå§‹åŒ–æ¨¡å‹åŠ è½½å™¨
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        """
        self.model_path = model_path
        self.model = None
        self.data_type = None
        self.config = None
        self.preprocessor = None
        self.emotion_lexicon = None
        
        # åŠ è½½æƒ…æ„Ÿè¯æ±‡è¡¨
        self.load_emotion_lexicon()
        
        # åŠ è½½æ¨¡å‹
        self.load_model()
    
    def load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        try:
            # åŠ è½½æ¨¡å‹æ–‡ä»¶
            checkpoint = torch.load(self.model_path, map_location=torch.device('cpu'))
            
            # æ‰“å°æ£€æŸ¥ç‚¹ä¸­çš„é”®åˆ—è¡¨
            print(f"æ£€æŸ¥ç‚¹ä¸­çš„é”®åˆ—è¡¨: {list(checkpoint.keys())}")
            print(f"æ£€æŸ¥ç‚¹ä¸­æ˜¯å¦åŒ…å«vocab: {'vocab' in checkpoint}")
            if 'vocab' in checkpoint:
                print(f"è¯æ±‡è¡¨å¤§å°: {len(checkpoint['vocab'])}")
                # æ‰“å°å‰10ä¸ªè¯æ±‡è¡¨æ¡ç›®
                print(f"è¯æ±‡è¡¨å‰10é¡¹: {list(checkpoint['vocab'].items())[:10]}")
            else:
                print("æ£€æŸ¥ç‚¹ä¸­ä¸åŒ…å«è¯æ±‡è¡¨")
            
            # è·å–æ¨¡å‹é…ç½®
            self.data_type = checkpoint.get('data_type', 'text')
            self.config = checkpoint.get('config', {})
            
            # ç¡®ä¿vocabè¢«æ­£ç¡®åŠ è½½ï¼Œæ— è®ºæ˜¯ä»checkpointç›´æ¥è·å–è¿˜æ˜¯ä»configä¸­è·å–
            vocab_loaded = False
            if 'vocab' in checkpoint:
                self.config['vocab'] = checkpoint['vocab']
                self.vocab = checkpoint['vocab']  # ä¿å­˜åˆ°self.vocabä¾›id_to_textä½¿ç”¨
                # è®¾ç½®æ­£ç¡®çš„vocab_sizeï¼ŒåŒ…å«ç‰¹æ®Šæ ‡è®°
                self.config['vocab_size'] = len(self.config['vocab'])
                print(f"ä»æ£€æŸ¥ç‚¹åŠ è½½äº†vocabï¼Œé•¿åº¦ä¸º: {self.config['vocab_size']}")
                print(f"è¯æ±‡è¡¨å‰10é¡¹: {list(self.vocab.items())[:10]}")
                vocab_loaded = True
            elif 'vocab' in checkpoint.get('config', {}):
                # å¦‚æœvocabåœ¨checkpointçš„configä¸­
                self.config['vocab'] = checkpoint['config']['vocab']
                self.vocab = self.config['vocab']
                self.config['vocab_size'] = len(self.config['vocab'])
                print(f"ä»æ£€æŸ¥ç‚¹çš„configä¸­åŠ è½½äº†vocabï¼Œé•¿åº¦ä¸º: {self.config['vocab_size']}")
                print(f"è¯æ±‡è¡¨å‰10é¡¹: {list(self.vocab.items())[:10]}")
                vocab_loaded = True
            else:
                # å¯»æ‰¾å…¶ä»–å¯èƒ½åŒ…å«vocabçš„é”®
                for key in checkpoint.keys():
                    # è·³è¿‡é”®ä¸º'vocab'çš„é¡¹ï¼Œé¿å…åµŒå¥—å­—å…¸é—®é¢˜
                    if key == 'vocab':
                        continue
                    if isinstance(checkpoint[key], dict) and '<PAD>' in checkpoint[key]:
                        # è¿‡æ»¤æ‰æ‰€æœ‰é”®ä¸æ˜¯å­—ç¬¦ä¸²æˆ–æ•´æ•°çš„é¡¹ï¼Œé¿å…åµŒå¥—å­—å…¸é—®é¢˜
                        filtered_vocab = {k: v for k, v in checkpoint[key].items() if isinstance(k, (str, int))}
                        self.config['vocab'] = filtered_vocab
                        self.vocab = self.config['vocab']
                        self.config['vocab_size'] = len(self.config['vocab'])
                        print(f"ä»æ£€æŸ¥ç‚¹çš„{key}ä¸­åŠ è½½äº†vocabï¼Œé•¿åº¦ä¸º: {self.config['vocab_size']}")
                        print(f"è¯æ±‡è¡¨å‰10é¡¹: {list(self.vocab.items())[:10]}")
                        vocab_loaded = True
                        break
                
                # å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°vocabï¼Œå°è¯•ä»æ¨¡å‹æƒé‡ä¸­è·å–æ­£ç¡®çš„vocab_size
                if not vocab_loaded:
                    if 'model_state_dict' in checkpoint and 'embedding.weight' in checkpoint['model_state_dict']:
                        self.config['vocab_size'] = checkpoint['model_state_dict']['embedding.weight'].shape[0]
                        print(f"ä»æ¨¡å‹æƒé‡è·å–vocab_size: {self.config['vocab_size']}")
                    else:
                        # è®¾ç½®ä¸€ä¸ªé»˜è®¤å€¼
                        self.config['vocab_size'] = 10000
                        print(f"ä½¿ç”¨é»˜è®¤vocab_size: {self.config['vocab_size']}")
            
            # æ ¹æ®æ•°æ®ç±»å‹åˆ›å»ºé¢„å¤„ç†æ¨¡å—
            if self.data_type == 'text':
                self.preprocessor = DataPreprocessor({'preprocess': {'max_length': self.config.get('max_length', 100)}})
            elif self.data_type in ['image', 'video']:
                self.preprocessor = DataPreprocessor({'preprocess': {'image': {'size': self.config.get('image_size', (224, 224))}}})
            
            # æ‰“å°é…ç½®ä¿¡æ¯ï¼Œè°ƒè¯•æ¨¡å‹åŠ è½½é—®é¢˜
            print(f"é…ç½®ä¿¡æ¯: {self.config}")
            print(f"é…ç½®ä¸­æ˜¯å¦åŒ…å«vocab: {'vocab' in self.config}")
            if 'vocab' in self.config:
                print(f"vocabé•¿åº¦: {len(self.config['vocab'])}")
            print(f"é…ç½®ä¸­æ˜¯å¦åŒ…å«vocab_size: {'vocab_size' in self.config}")
            if 'vocab_size' in self.config:
                print(f"vocab_size: {self.config['vocab_size']}")
            
            # æ£€æµ‹æ¨¡å‹ç±»å‹
            model_type = self.config.get('model_type', None)
            
            # å¦‚æœé…ç½®ä¸­æ²¡æœ‰model_typeï¼Œå°è¯•ä»æ¨¡å‹çŠ¶æ€å­—å…¸ä¸­æ£€æµ‹
            if model_type is None and 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯Seq2Seq Transformeræ¨¡å‹ï¼ˆåŒ…å«encoderå’Œdecoderï¼‰
                if any(key.startswith('transformer_encoder') for key in state_dict.keys()) and any(key.startswith('transformer_decoder') for key in state_dict.keys()):
                    model_type = 'seq2seq_transformer'
                    print(f"ä»æ¨¡å‹çŠ¶æ€å­—å…¸æ£€æµ‹åˆ°æ¨¡å‹ç±»å‹: {model_type}")
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ™®é€šTransformeræ¨¡å‹ï¼ˆåªåŒ…å«encoderï¼‰
                elif any(key.startswith('transformer_encoder') for key in state_dict.keys()):
                    model_type = 'transformer'
                    print(f"ä»æ¨¡å‹çŠ¶æ€å­—å…¸æ£€æµ‹åˆ°æ¨¡å‹ç±»å‹: {model_type}")
                else:
                    model_type = 'simple_classifier'
                    print(f"æ— æ³•ä»æ¨¡å‹çŠ¶æ€å­—å…¸ç¡®å®šæ¨¡å‹ç±»å‹ï¼Œä½¿ç”¨é»˜è®¤ç±»å‹: {model_type}")
            elif model_type is None:
                model_type = 'simple_classifier'
                print(f"æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹ç±»å‹ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤ç±»å‹: {model_type}")
            
            # ä¿å­˜model_typeåˆ°å®ä¾‹å˜é‡
            self.model_type = model_type
            
            # ç¡®ä¿åµŒå…¥ç»´åº¦ä¸ä¿å­˜çš„æ¨¡å‹åŒ¹é…
            if 'model_state_dict' in checkpoint and 'embedding.weight' in checkpoint['model_state_dict']:
                embedding_dim = checkpoint['model_state_dict']['embedding.weight'].shape[1]
                self.config['embedding_dim'] = embedding_dim
                print(f"ä»æ¨¡å‹çŠ¶æ€å­—å…¸è·å–embedding_dim: {embedding_dim}")
            
            # ä»æ¨¡å‹çŠ¶æ€å­—å…¸ä¸­æå–å…¶ä»–é…ç½®å‚æ•°
            if 'model_state_dict' in checkpoint and model_type in ['transformer', 'seq2seq_transformer']:
                state_dict = checkpoint['model_state_dict']
                
                # æå–max_lengthï¼ˆä»pos_encoder.peçš„å½¢çŠ¶ä¸­è·å–ï¼‰
                if 'pos_encoder.pe' in state_dict:
                    max_length = state_dict['pos_encoder.pe'].shape[0]
                    self.config['max_length'] = max_length
                    print(f"ä»æ¨¡å‹çŠ¶æ€å­—å…¸è·å–max_length: {max_length}")
                
                # æå–hidden_dimï¼ˆä»linear1.weightçš„å½¢çŠ¶ä¸­è·å–ï¼‰
                if 'transformer_encoder.layers.0.linear1.weight' in state_dict:
                    hidden_dim = state_dict['transformer_encoder.layers.0.linear1.weight'].shape[0]
                    self.config['hidden_dim'] = hidden_dim
                    print(f"ä»æ¨¡å‹çŠ¶æ€å­—å…¸è·å–hidden_dim: {hidden_dim}")
                
                # æå–num_classesæˆ–vocab_sizeï¼ˆä»fc.weightçš„å½¢çŠ¶ä¸­è·å–ï¼‰
                if 'fc.weight' in state_dict:
                    if model_type == 'seq2seq_transformer':
                        # å¯¹äºåºåˆ—åˆ°åºåˆ—æ¨¡å‹ï¼Œfc.weight.shape[0]æ˜¯vocab_size
                        vocab_size = state_dict['fc.weight'].shape[0]
                        self.config['vocab_size'] = vocab_size
                        print(f"ä»æ¨¡å‹çŠ¶æ€å­—å…¸è·å–vocab_size: {vocab_size}")
                    else:
                        # å¯¹äºåˆ†ç±»æ¨¡å‹ï¼Œfc.weight.shape[0]æ˜¯num_classes
                        num_classes = state_dict['fc.weight'].shape[0]
                        self.config['num_classes'] = num_classes
                        print(f"ä»æ¨¡å‹çŠ¶æ€å­—å…¸è·å–num_classes: {num_classes}")
                
                # æå–num_layersï¼ˆä»transformer_encoder.layersçš„æ•°é‡ä¸­è·å–ï¼‰
                layer_count = 0
                while f'transformer_encoder.layers.{layer_count}.self_attn.in_proj_weight' in state_dict:
                    layer_count += 1
                if layer_count > 0:
                    self.config['num_layers'] = layer_count
                    print(f"ä»æ¨¡å‹çŠ¶æ€å­—å…¸è·å–num_layers: {layer_count}")
                
                # æå–num_headsï¼ˆä»self_attn.in_proj_weightçš„å½¢çŠ¶ä¸­è·å–ï¼‰
                if 'transformer_encoder.layers.0.self_attn.in_proj_weight' in state_dict:
                    # in_proj_weightçš„å½¢çŠ¶æ˜¯[3 * embed_dim, embed_dim]
                    # å…¶ä¸­embed_dimæ˜¯embedding_dim
                    in_proj_weight_shape = state_dict['transformer_encoder.layers.0.self_attn.in_proj_weight'].shape[0]
                    if in_proj_weight_shape % embedding_dim == 0:
                        num_heads = in_proj_weight_shape // (3 * embedding_dim)
                        self.config['num_heads'] = num_heads
                        print(f"ä»æ¨¡å‹çŠ¶æ€å­—å…¸è·å–num_heads: {num_heads}")
            
            # åˆ›å»ºæ¨¡å‹
            model_factory = ModelFactory()
            self.model = model_factory.create_model(model_type, self.config)
            
            # åŠ è½½æ¨¡å‹æƒé‡
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model.eval()
            
            print(f"æ¨¡å‹åŠ è½½æˆåŠŸï¼æ•°æ®ç±»å‹: {self.data_type}")
            
        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def preprocess_input(self, input_data: Union[str, np.ndarray]) -> Union[torch.Tensor, List[torch.Tensor]]:
        """é¢„å¤„ç†è¾“å…¥æ•°æ®
        
        Args:
            input_data: è¾“å…¥æ•°æ®
            
        Returns:
            é¢„å¤„ç†åçš„æ•°æ®
        """
        if self.data_type == 'text':
            # æ–‡æœ¬æ•°æ®é¢„å¤„ç†
            preprocessed = self.preprocessor.preprocess_text([input_data])
            return preprocessed
        elif self.data_type == 'image':
            # å›¾åƒæ•°æ®é¢„å¤„ç†
            preprocessed = self.preprocessor.preprocess_images([input_data])
            return preprocessed[0] if preprocessed else None
        elif self.data_type == 'video':
            # è§†é¢‘æ•°æ®é¢„å¤„ç†
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå‡è®¾è§†é¢‘æ˜¯å¸§åˆ—è¡¨
            preprocessed = self.preprocessor.preprocess_videos([('temp_video', [input_data])])
            return preprocessed[0] if preprocessed else None
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {self.data_type}")
    
    def predict(self, input_data: Union[str, np.ndarray], emotion: int = 0) -> Dict[str, Any]:
        """ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œç”Ÿæˆé€‚åˆé—®ç­”äº¤äº’çš„è¾“å‡ºï¼Œæ”¯æŒæƒ…æ„Ÿæ§åˆ¶
        
        Args:
            input_data: è¾“å…¥æ•°æ®ï¼Œå¯ä»¥æ˜¯æ–‡æœ¬æˆ–å›¾åƒ
            emotion: æƒ…æ„Ÿç±»åˆ« (0-4)ï¼Œ0=ç§¯æï¼Œ1=æ¶ˆæï¼Œ2=æ„¤æ€’ï¼Œ3=æƒŠè®¶ï¼Œ4=ä¸­æ€§
            
        Returns:
            é¢„æµ‹ç»“æœï¼ŒåŒ…å«å¯¹è¯å¼å›å¤
        """
        try:
            # é¢„å¤„ç†æ•°æ®
            preprocessed_data = self.preprocess_input(input_data)
            
            if preprocessed_data is None:
                return {'error': 'æ•°æ®é¢„å¤„ç†å¤±è´¥'}
            
            # å‡†å¤‡æ¨¡å‹è¾“å…¥
            if self.data_type == 'text':
                # æ–‡æœ¬æ•°æ®éœ€è¦é¢å¤–å¤„ç†
                vocab = self.config.get('vocab', {})
                max_length = self.config.get('max_length', 100)
                
                # å°†æ–‡æœ¬è½¬æ¢ä¸ºç´¢å¼•
                vector = [vocab.get(token, vocab.get('<UNK>', 1)) for token in preprocessed_data[0]]
                
                # æˆªæ–­æˆ–å¡«å……åˆ°æœ€å¤§é•¿åº¦
                if len(vector) < max_length:
                    vector += [vocab.get('<PAD>', 0)] * (max_length - len(vector))
                else:
                    vector = vector[:max_length]
                
                input_tensor = torch.tensor([vector], dtype=torch.long)
                
                # æ£€æŸ¥æ¨¡å‹ç±»å‹
                if hasattr(self.model, 'generate'):
                    # ç”Ÿæˆå¼æ¨¡å‹
                    # ä½¿ç”¨æ­£ç¡®çš„ç‰¹æ®Šæ ‡è®°ä½œä¸ºå¼€å§‹å’Œç»“æŸæ ‡è®°
                    start_token = 2  # <SOS>
                    end_token = 3    # <EOS>
                    
                    # ç”Ÿæˆå›å¤ï¼Œæ”¯æŒæƒ…æ„Ÿæ§åˆ¶
                    if hasattr(self.model, 'generate_with_emotion'):
                        generated_ids = self.model.generate_with_emotion(input_tensor, emotion=emotion, start_token=start_token, end_token=end_token)
                    else:
                        generated_ids = self.model.generate(input_tensor, start_token=start_token, end_token=end_token)
                    
                    # è°ƒè¯•ä¿¡æ¯ï¼šæŸ¥çœ‹ç”Ÿæˆçš„IDåºåˆ—
                    print(f"ç”Ÿæˆçš„IDåºåˆ—: {generated_ids[0].tolist()}")
                    print(f"self.vocabå­˜åœ¨æ€§: {hasattr(self, 'vocab') and self.vocab is not None}")
                    if hasattr(self, 'vocab') and self.vocab is not None:
                        print(f"è¯æ±‡è¡¨å¤§å°: {len(self.vocab)}")
                        # æ£€æŸ¥ç‰¹æ®Šæ ‡è®°çš„ID
                        print(f"<PAD> ID: {self.vocab.get('<PAD>')}")
                        print(f"<UNK> ID: {self.vocab.get('<UNK>')}")
                        print(f"<SOS> ID: {self.vocab.get('<SOS>')}")
                        print(f"<EOS> ID: {self.vocab.get('<EOS>')}")
                    
                    # å°†ç”Ÿæˆçš„IDè½¬æ¢ä¸ºæ–‡æœ¬
                    generated_text = self.id_to_text(generated_ids[0].tolist())
                    print(f"è½¬æ¢åçš„æ–‡æœ¬: '{generated_text}'")
                    
                    # æ·»åŠ æƒ…æ„Ÿè¯æ±‡ï¼Œä½¿å›å¤æ›´æœ‰æ„Ÿæƒ…
                    emotional_text = self.add_emotion_to_text(generated_text, emotion)
                    
                    return {
                        "generated_text": generated_text,
                        "emotional_text": emotional_text,
                        "reply": emotional_text,
                        "emotion": emotion
                    }
                else:
                    # åˆ†ç±»æ¨¡å‹
                    with torch.no_grad():
                        outputs = self.model(input_tensor)
                        probabilities = torch.nn.functional.softmax(outputs, dim=1)
                        predicted_class = torch.argmax(probabilities, dim=1).item()
                        confidence = probabilities[0, predicted_class].item()
                    
                    # ç”Ÿæˆå¯¹è¯å¼å›å¤
                    reply = f"æ ¹æ®åˆ†æï¼Œæˆ‘é¢„æµ‹è¯¥è¾“å…¥å±äºç±»åˆ« {predicted_class}ï¼Œç½®ä¿¡åº¦ä¸º {confidence:.2f}ã€‚"
                    
                    return {
                        "predicted_class": predicted_class,
                        "confidence": confidence,
                        "probabilities": probabilities.tolist()[0],
                        "reply": reply
                    }
            else:
                # å›¾åƒæˆ–è§†é¢‘æ•°æ®
                if not isinstance(preprocessed_data, torch.Tensor):
                    return {'error': 'æ•°æ®é¢„å¤„ç†å¤±è´¥'}
                input_tensor = preprocessed_data.unsqueeze(0)
                
                # è¿›è¡Œé¢„æµ‹
                with torch.no_grad():
                    outputs = self.model(input_tensor)
                    probabilities = torch.nn.functional.softmax(outputs, dim=1)
                    predicted_class = torch.argmax(probabilities, dim=1).item()
                    confidence = probabilities[0, predicted_class].item()
                
                # ç”Ÿæˆå¯¹è¯å¼å›å¤
                reply = f"æ ¹æ®å›¾åƒåˆ†æï¼Œæˆ‘é¢„æµ‹è¯¥å›¾åƒå±äºç±»åˆ« {predicted_class}ï¼Œç½®ä¿¡åº¦ä¸º {confidence:.2f}ã€‚"
                
                return {
                    "predicted_class": predicted_class,
                    "confidence": confidence,
                    "probabilities": probabilities.tolist()[0],
                    "reply": reply
                }
            
        except Exception as e:
            error_msg = f"é¢„æµ‹å¤±è´¥: {str(e)}"
            print(error_msg)
            return {'error': error_msg}
    
    def id_to_text(self, ids: List[int]) -> str:
        """å°†idåºåˆ—è½¬æ¢ä¸ºæ–‡æœ¬
        
        Args:
            ids: idåˆ—è¡¨
            
        Returns:
            æ–‡æœ¬
        """
        # åˆ›å»ºåå‘è¯æ±‡è¡¨
        if hasattr(self, 'vocab') and self.vocab is not None:
            try:
                # æ£€æŸ¥vocabç»“æ„æ˜¯å¦æ­£å¸¸
                if isinstance(self.vocab, dict) and all(isinstance(k, (str, int)) for k in self.vocab.keys()):
                    reverse_vocab = {v: k for k, v in self.vocab.items()}
                    
                    # è½¬æ¢idä¸ºæ–‡æœ¬
                    text = []
                    for idx in ids:
                        # è·³è¿‡ç‰¹æ®Šæ ‡è®°
                        if idx in reverse_vocab and reverse_vocab[idx] not in ["<PAD>", "<UNK>", "<SOS>", "<EOS>"]:
                            text.append(reverse_vocab[idx])
                    
                    return " ".join(text)
                else:
                    print(f"è­¦å‘Šï¼šè¯æ±‡è¡¨ç»“æ„å¼‚å¸¸ï¼Œç±»å‹: {type(self.vocab)}, é”®ç±»å‹: {[type(k) for k in list(self.vocab.keys())[:5]]}")
                    # å¦‚æœè¯æ±‡è¡¨ç»“æ„å¼‚å¸¸ï¼Œå°è¯•ä»åµŒå…¥å±‚æƒé‡è·å–è¯æ±‡è¡¨å¤§å°
                    if hasattr(self, 'config') and 'vocab_size' in self.config:
                        print(f"ä½¿ç”¨vocab_size: {self.config['vocab_size']}")
            except Exception as e:
                print(f"åˆ›å»ºåå‘è¯æ±‡è¡¨æ—¶å‡ºé”™: {e}")
                print(f"è¯æ±‡è¡¨ç±»å‹: {type(self.vocab)}")
                # æ‰“å°è¯æ±‡è¡¨çš„ä¸€äº›å†…å®¹ï¼Œä»¥ä¾¿è°ƒè¯•
                if hasattr(self.vocab, 'items'):
                    items = list(self.vocab.items())[:10]
                    print(f"è¯æ±‡è¡¨å‰10é¡¹: {items}")
        
        # å¦‚æœæ²¡æœ‰è¯æ±‡è¡¨æˆ–åˆ›å»ºåå‘è¯æ±‡è¡¨å¤±è´¥ï¼Œç®€å•åœ°è¿”å›idçš„å­—ç¬¦ä¸²è¡¨ç¤º
        return " ".join([str(id) for id in ids])
    
    def add_emotion_to_text(self, text: str, emotion: int = 0) -> str:
        """ä¸ºæ–‡æœ¬æ·»åŠ æƒ…æ„Ÿè¯æ±‡ï¼Œä½¿å›å¤æ›´æœ‰æ„Ÿæƒ…
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            emotion: æƒ…æ„Ÿç±»åˆ« (0-4)
            
        Returns:
            æ·»åŠ äº†æƒ…æ„Ÿçš„æ–‡æœ¬
        """
        if not text:
            return text
            
        # æƒ…æ„Ÿå‰ç¼€
        emotion_prefixes = {
            0: ["ğŸ˜Š å¾ˆé«˜å…´åœ°å‘Šè¯‰ä½ ï¼Œ", "ğŸ˜„ å¼€å¿ƒåœ°è¯´ï¼Œ", "ğŸ¥° æ„‰æ‚¦åœ°åˆ†äº«ï¼Œ", "ğŸ˜Š å–œæ‚¦åœ°å‘ŠçŸ¥ï¼Œ", "ğŸ˜„ å…´å¥‹åœ°è¯´ï¼Œ"],
            1: ["ğŸ˜” é—æ†¾åœ°è¯´ï¼Œ", "ğŸ˜¢ éš¾è¿‡åœ°è¡¨ç¤ºï¼Œ", "ğŸ˜ æ‚²ä¼¤åœ°å›ç­”ï¼Œ", "ğŸ˜” æƒ‹æƒœåœ°è¯´ï¼Œ", "ğŸ˜¢ æ²®ä¸§åœ°å›åº”ï¼Œ"],
            2: ["ğŸ˜  æ„¤æ€’åœ°æŒ‡å‡ºï¼Œ", "ğŸ˜¤ æ¼ç«åœ°å›åº”ï¼Œ", "ğŸ˜¡ æ°”æ„¤åœ°è¯´ï¼Œ", "ğŸ˜  ç”Ÿæ°”åœ°è¡¨ç¤ºï¼Œ", "ğŸ˜¤ æ¼æ€’åœ°è¯´ï¼Œ"],
            3: ["ğŸ˜® æƒŠè®¶åœ°å‘ç°ï¼Œ", "ğŸ˜² åƒæƒŠåœ°è¡¨ç¤ºï¼Œ", "ğŸ˜¯ æ„å¤–åœ°è¯´ï¼Œ", "ğŸ˜® éœ‡æƒŠåœ°å›åº”ï¼Œ", "ğŸ˜² æ„•ç„¶åœ°è¯´ï¼Œ"],
            4: ["ğŸ˜ å¹³é™åœ°å‘Šè¯‰ä½ ï¼Œ", "ğŸ˜Œ æ·¡å®šåœ°è¡¨ç¤ºï¼Œ", "ğŸ¤” ç†æ€§åœ°åˆ†æï¼Œ", "ğŸ˜ å®¢è§‚åœ°è¯´ï¼Œ", "ğŸ˜Œ å¹³å’Œåœ°å›åº”ï¼Œ"]
        }
        
        # æƒ…æ„Ÿåç¼€
        emotion_suffixes = {
            0: ["ï¼ğŸ˜Š", "å“¦ï¼ğŸ˜„", "å‘€ï¼ğŸ¥°", "å‘¢ï¼ğŸ˜Š", "å“¦ï¼ğŸ¥°"],
            1: ["ã€‚ğŸ˜”", "...ğŸ˜¢", "å‘¢ã€‚ğŸ˜", "å“¦ã€‚ğŸ˜”", "å”‰...ğŸ˜¢"],
            2: ["ï¼ğŸ˜ ", "ï¼ğŸ˜¤", "ï¼ğŸ˜¡", "ï¼ğŸ˜ ", "ï¼ğŸ˜¤"],
            3: ["ï¼ğŸ˜®", "ï¼ğŸ˜²", "ï¼ğŸ˜¯", "å‘¢ï¼ğŸ˜®", "å“¦ï¼ğŸ˜²"],
            4: ["ã€‚ğŸ˜", "ã€‚ğŸ˜Œ", "ã€‚ğŸ¤”", "å“¦ã€‚ğŸ˜", "å‘¢ã€‚ğŸ˜Œ"]
        }
        
        # éšæœºé€‰æ‹©å‰ç¼€å’Œåç¼€
        import random
        prefix = random.choice(emotion_prefixes.get(emotion, [""]))
        suffix = random.choice(emotion_suffixes.get(emotion, [""]))
        
        # å¦‚æœæ–‡æœ¬å·²ç»æœ‰æ ‡ç‚¹ç¬¦å·ç»“å°¾ï¼Œå»æ‰åæ·»åŠ æƒ…æ„Ÿåç¼€
        if text and text[-1] in ["ã€‚", "ï¼", "ï¼Ÿ", "..."]:
            text = text[:-1]
        
        return f"{prefix}{text}{suffix}"
    
    def load_emotion_lexicon(self):
        """åŠ è½½æƒ…æ„Ÿè¯æ±‡è¡¨"""
        lexicon_path = "d:\\linai\\data\\emotion_lexicon.txt"
        self.emotion_lexicon = {
            0: [],  # ç§¯æ
            1: [],  # æ¶ˆæ
            2: [],  # æ„¤æ€’
            3: [],  # æƒŠè®¶
            4: []   # ä¸­æ€§
        }
        
        try:
            with open(lexicon_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line or line.startswith('#'):
                        continue
                    parts = line.split(',')
                    if len(parts) < 3:
                        continue
                    # æ–‡ä»¶æ ¼å¼æ˜¯ï¼šæƒ…æ„Ÿç±»åˆ«,è¯æ±‡,æƒé‡
                    emotion = int(parts[0].strip())
                    word = parts[1].strip()
                    weight = float(parts[2].strip())
                    if emotion in self.emotion_lexicon:
                        self.emotion_lexicon[emotion].append((word, weight))
            print(f"æƒ…æ„Ÿè¯æ±‡è¡¨åŠ è½½æˆåŠŸï¼ŒåŒ…å« {sum(len(words) for words in self.emotion_lexicon.values())} ä¸ªè¯æ±‡")
        except Exception as e:
            print(f"åŠ è½½æƒ…æ„Ÿè¯æ±‡è¡¨å¤±è´¥: {e}")
            # å¦‚æœåŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤çš„è¯æ±‡è¡¨
            self.emotion_lexicon = {
                0: [("å¥½", 1.0), ("æ£’", 1.0), ("ä¼˜ç§€", 1.0), ("å–œæ¬¢", 1.0), ("å¼€å¿ƒ", 1.0), ("å¿«ä¹", 1.0), ("é«˜å…´", 1.0), ("æ»¡æ„", 1.0), ("èµ", 1.0), ("ç²¾å½©", 1.0)],
                1: [("å", 1.0), ("å·®", 1.0), ("ç³Ÿç³•", 1.0), ("è®¨åŒ", 1.0), ("æ‚²ä¼¤", 1.0), ("éš¾è¿‡", 1.0), ("ç”Ÿæ°”", 1.0), ("å¤±æœ›", 1.0)],
                2: [("æ„¤æ€’", 1.0), ("ç”Ÿæ°”", 1.0), ("æ¼ç«", 1.0), ("æ¼æ€’", 1.0), ("æ°”æ„¤", 1.0), ("æš´æ€’", 1.0), ("æ„¤æ¨", 1.0), ("æ„¤æ…¨", 1.0)],
                3: [("æƒŠè®¶", 1.0), ("åƒæƒŠ", 1.0), ("éœ‡æƒŠ", 1.0), ("è¯§å¼‚", 1.0), ("æ„å¤–", 1.0), ("æ„•ç„¶", 1.0), ("æƒŠå¹", 1.0), ("è®¶å¼‚", 1.0)],
                4: [("ä¸€èˆ¬", 1.0), ("æ™®é€š", 1.0), ("æ­£å¸¸", 1.0), ("å¹³å¸¸", 1.0)]
            }
    
    def analyze_emotion(self, text: str) -> int:
        """åˆ†ææ–‡æœ¬çš„æƒ…æ„Ÿ
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            æƒ…æ„Ÿç±»åˆ« (0-4)
        """
        if not text:
            return 4  # ä¸­æ€§
            
        # å¦‚æœæƒ…æ„Ÿè¯æ±‡è¡¨å·²åŠ è½½ï¼Œä½¿ç”¨è¯æ±‡è¡¨è¿›è¡Œåˆ†æ
        if self.emotion_lexicon:
            emotion_scores = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}
            
            # é¢„å¤„ç†æ–‡æœ¬
            text_lower = text.lower()
            tokens = text_lower.split()
            
            # è®¡ç®—æ¯ä¸ªæƒ…æ„Ÿç±»åˆ«çš„å¾—åˆ†
            for emotion, words_with_weights in self.emotion_lexicon.items():
                for word, weight in words_with_weights:
                    if word in text_lower:
                        emotion_scores[emotion] += weight
            
            # è€ƒè™‘å¦å®šè¯çš„å½±å“
            negative_words = ["ä¸", "æ²¡", "æ— ", "å¦", "é", "æœª", "åˆ«", "è«", "ä¼‘", "å‹¿"]
            for i, token in enumerate(tokens):
                if token in negative_words and i < len(tokens) - 1:
                    next_token = tokens[i + 1]
                    # æ£€æŸ¥ä¸‹ä¸€ä¸ªè¯æ˜¯å¦åœ¨ä»»ä½•æƒ…æ„Ÿç±»åˆ«ä¸­
                    for emotion, words_with_weights in self.emotion_lexicon.items():
                        for word, weight in words_with_weights:
                            if word == next_token:
                                # å¦å®šè¯åè½¬æƒ…æ„Ÿï¼šç§¯æå˜æ¶ˆæï¼Œæ¶ˆæå˜ç§¯æï¼Œä¿æŒå…¶ä»–æƒ…æ„Ÿä¸å˜
                                if emotion == 0:  # ç§¯æ -> æ¶ˆæ
                                    emotion_scores[1] += weight
                                    emotion_scores[0] -= weight
                                elif emotion == 1:  # æ¶ˆæ -> ç§¯æ
                                    emotion_scores[0] += weight
                                    emotion_scores[1] -= weight
            
            # æ‰¾å‡ºå¾—åˆ†æœ€é«˜çš„æƒ…æ„Ÿç±»åˆ«
            max_score = max(emotion_scores.values())
            if max_score > 0:
                return max(emotion_scores, key=emotion_scores.get)
            else:
                return 4  # ä¸­æ€§
        else:
            # ä½¿ç”¨ç®€å•çš„å…³é”®è¯åŒ¹é…ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰ï¼Œè€ƒè™‘å¦å®šè¯
            positive_keywords = ["å¥½", "å–œæ¬¢", "æ£’", "ä¼˜ç§€", "é«˜å…´", "å¿«ä¹", "æ»¡æ„", "èµ", "ç²¾å½©", "å®Œç¾", "å¼€å¿ƒ", "æ„‰æ‚¦"]
            negative_keywords = ["ä¸å¥½", "ä¸å–œæ¬¢", "å·®", "ç³Ÿç³•", "ç”Ÿæ°”", "éš¾è¿‡", "å¤±æœ›", "å", "æ¶å¿ƒ", "è®¨åŒ", "ç—›è‹¦", "æ‚²ä¼¤"]
            angry_keywords = ["æ„¤æ€’", "ç”Ÿæ°”", "æ¼ç«", "æ¼æ€’", "æ°”æ„¤", "æš´æ€’", "æ„¤æ¨", "æ„¤æ…¨"]
            surprise_keywords = ["æƒŠè®¶", "åƒæƒŠ", "éœ‡æƒŠ", "è¯§å¼‚", "æ„å¤–", "æ„•ç„¶", "æƒŠå¹"]
            negative_words = ["ä¸", "æ²¡", "æ— ", "å¦", "é", "æœª", "åˆ«", "è«", "ä¼‘", "å‹¿"]
            
            text_lower = text.lower()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¦å®šè¯
            has_negative = any(word in text_lower for word in negative_words)
            
            if has_negative:
                # å¦‚æœæœ‰å¦å®šè¯ï¼Œåè½¬ç§¯æ/æ¶ˆæåˆ¤æ–­
                if any(keyword in text_lower for keyword in negative_keywords):
                    return 0  # æœ‰å¦å®šè¯ + æ¶ˆæå…³é”®è¯ = ç§¯æ
                elif any(keyword in text_lower for keyword in positive_keywords):
                    return 1  # æœ‰å¦å®šè¯ + ç§¯æå…³é”®è¯ = æ¶ˆæ
                elif any(keyword in text_lower for keyword in angry_keywords):
                    return 2  # æ„¤æ€’ä¸å—å¦å®šè¯å½±å“
                elif any(keyword in text_lower for keyword in surprise_keywords):
                    return 3  # æƒŠè®¶ä¸å—å¦å®šè¯å½±å“
                else:
                    return 4  # ä¸­æ€§
            else:
                # æ²¡æœ‰å¦å®šè¯ï¼Œæ­£å¸¸åˆ¤æ–­
                if any(keyword in text_lower for keyword in angry_keywords):
                    return 2  # æ„¤æ€’
                elif any(keyword in text_lower for keyword in surprise_keywords):
                    return 3  # æƒŠè®¶
                elif any(keyword in text_lower for keyword in positive_keywords):
                    return 0  # ç§¯æ
                elif any(keyword in text_lower for keyword in negative_keywords):
                    return 1  # æ¶ˆæ
                else:
                    return 4  # ä¸­æ€§
